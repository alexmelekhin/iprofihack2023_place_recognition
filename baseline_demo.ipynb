{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/alexmelekhin/iprofihack2023_place_recognition/blob/dev/baseline_demo.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка в Google Colab\n",
    "\n",
    "Ячейки ниже рекомендуется использовать для установки зависимостей в Google Colab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Убедитесь, что вы подключены к окружению с GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. По умолчанию установлена версия torch 2.0, нам нужно откатиться до 1.13.1 (код не тестировался на версии 2.0 и может вести себя непредсказуемо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchvision==0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available in torch?: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Установка необходимых библиотек для сборки MinkowskiEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install libopenblas-dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Сборка и установка MinkowskiEngine из исходников (занимает много времени)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/NVIDIA/MinkowskiEngine -v --no-deps \\\n",
    "                          --install-option=\"--force_cuda\" \\\n",
    "                          --install-option=\"--blas=openblas\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Проверка, что все работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available in torch?: {torch.cuda.is_available()}\")\n",
    "import MinkowskiEngine as ME\n",
    "print(f\"Is CUDA available in MinkowskiEngine?: {ME.is_cuda_available()}\")\n",
    "ME.print_diagnostics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Финальный шаг - установка библиотеки [opr](https://github.com/alexmelekhin/open_place_recognition), код из которой будет использоваться в бейзлайне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexmelekhin/open_place_recognition\n",
    "%cd open_place_recognition\n",
    "!pip install -e .  # флаг -e необходим для возможности редактировать код уже установленной библиотеки\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета в Google Colab\n",
    "\n",
    "Пример кода для загрузки датасета."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете воспользоваться утилитой gdown, которая по умолчанию доступна в Colab. Допустим, https://drive.google.com/file/d/1EdOTVgBJxsNUMecne7Fs4obJdJnDuJ18/view?usp=share_link - ссылка на файл. Чтобы скачать его, нам нужно передать в gdown в качестве аргумента его id - для данного примера это `1EdOTVgBJxsNUMecne7Fs4obJdJnDuJ18` (часть ссылки между `file/d/` и `/view`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1EdOTVgBJxsNUMecne7Fs4obJdJnDuJ18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете сверить хэш-сумму файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sha256sum public.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И распаковать архив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q public.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовое решение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите веса MinkLoc++, предобученного на датасете Oxford RobotCar по ссылке: https://drive.google.com/file/d/1zlfdX217Nh3_QL5r0XAHUjDFjIPxUmMg/view?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если вы в colab'е:\n",
    "# !gdown 1zlfdX217Nh3_QL5r0XAHUjDFjIPxUmMg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker_hackathon/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from opr.models import minkloc_multimodal\n",
    "\n",
    "model = minkloc_multimodal(weights=\"baseline_minkloc_multimodal.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "IMAGE_LR = 0.0001\n",
    "CLOUD_LR = 0.001\n",
    "FUSION_LR = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "SCHEDULER_GAMMA = 0.1\n",
    "SCHEDULER_STEPS = [5]\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_EXPANSION_TH = None\n",
    "CHECKPOINTS_DIR = Path(\"checkpoints\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для инициализации функции лосса предлагается воспользоваться средствами библиотеки [Hydra](https://hydra.cc/docs/intro/).\n",
    "\n",
    "Примеры готовых конфиг-файлов есть в директории \"configs\" [репозитория opr](https://github.com/alexmelekhin/open_place_recognition). Обратите внимание, что в конфигурации датасета необходимо указать путь к его директории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from opr.datasets.dataloader_factory import make_dataloaders\n",
    "\n",
    "LOSS_CFG_PATH = ...\n",
    "DATASET_CFG_PATH = ...\n",
    "\n",
    "loss_cfg = OmegaConf.load(LOSS_CFG_PATH)\n",
    "loss_fn = instantiate(loss_cfg)\n",
    "\n",
    "dataset_cfg = OmegaConf.load(DATASET_CFG_PATH)\n",
    "\n",
    "dataset_cfg.dataset.dataset_root = ...  # change path\n",
    "\n",
    "dataloaders = make_dataloaders(\n",
    "    dataset_cfg=dataset_cfg.dataset,\n",
    "    batch_sampler_cfg=dataset_cfg.sampler,\n",
    "    num_workers=dataset_cfg.num_workers,\n",
    ")\n",
    "\n",
    "params_list = []\n",
    "if model.image_module is not None and IMAGE_LR is not None:\n",
    "    params_list.append({\"params\": model.image_module.parameters(), \"lr\": IMAGE_LR})\n",
    "if model.cloud_module is not None and CLOUD_LR is not None:\n",
    "    params_list.append({\"params\": model.cloud_module.parameters(), \"lr\": CLOUD_LR})\n",
    "if model.fusion_module is not None and FUSION_LR is not None:\n",
    "    params_list.append({\"params\": model.fusion_module.parameters(), \"lr\": FUSION_LR})\n",
    "optimizer = Adam(params_list, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = MultiStepLR(optimizer, milestones=SCHEDULER_STEPS, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "if not CHECKPOINTS_DIR.exists():\n",
    "    CHECKPOINTS_DIR.mkdir(parents=True)\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating test set descriptors: 100%|██████████| 116/116 [00:17<00:00,  6.67it/s]\n",
      "Calculating metrics: 100%|██████████| 6/6 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@N:\n",
      "[0.35832433 0.46075096 0.52536504 0.5618218  0.59198689 0.61640536\n",
      " 0.63445118 0.65197783 0.67150134 0.68306623 0.69275496 0.70492458\n",
      " 0.71186098 0.71967074 0.72547996 0.73210702 0.73955678 0.74756834\n",
      " 0.75144274 0.75862737 0.76363231 0.77028684 0.77800951 0.78381246\n",
      " 0.78966836]\n",
      "Mean Recall@1% = 0.6164053595498168\n",
      "Mean top-1 distance = 1.0901976154417208\n",
      "[0.35832433 0.46075096 0.52536504 0.5618218  0.59198689 0.61640536\n",
      " 0.63445118 0.65197783 0.67150134 0.68306623 0.69275496 0.70492458\n",
      " 0.71186098 0.71967074 0.72547996 0.73210702 0.73955678 0.74756834\n",
      " 0.75144274 0.75862737 0.76363231 0.77028684 0.77800951 0.78381246\n",
      " 0.78966836]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from opr.testing import test\n",
    "\n",
    "\n",
    "recall_at_n, recall_at_one_percent, mean_top1_distance = test(\n",
    "    model=model,\n",
    "    descriptor_key=\"fusion\",\n",
    "    dataloader=dataloaders[\"test\"],\n",
    "    dist_thresh=5.0,\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35832433 0.46075096 0.52536504 0.5618218  0.59198689 0.61640536\n",
      " 0.63445118 0.65197783 0.67150134 0.68306623 0.69275496 0.70492458\n",
      " 0.71186098 0.71967074 0.72547996 0.73210702 0.73955678 0.74756834\n",
      " 0.75144274 0.75862737 0.76363231 0.77028684 0.77800951 0.78381246\n",
      " 0.78966836]\n"
     ]
    }
   ],
   "source": [
    "print(recall_at_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from opr.training import epoch_loop\n",
    "\n",
    "\n",
    "best_recall_at_1 = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n\\n=====> Epoch {epoch+1}:\")\n",
    "    train_batch_size = dataloaders[\"train\"].batch_sampler.batch_size\n",
    "\n",
    "    print(\"\\n=> Training:\\n\")\n",
    "\n",
    "    train_stats, train_rate_non_zero = epoch_loop(\n",
    "        dataloader=dataloaders[\"train\"],\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        phase=\"train\",\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    print(f\"\\ntrain_rate_non_zero = {train_rate_non_zero}\")\n",
    "\n",
    "    if BATCH_EXPANSION_TH is not None:\n",
    "        if BATCH_EXPANSION_TH == 1.0:\n",
    "            print(\"Batch expansion rate is set to every epoch. Increasing batch size.\")\n",
    "            dataloaders[\"train\"].batch_sampler.expand_batch()\n",
    "        elif train_rate_non_zero is None:\n",
    "            print(\n",
    "                \"\\nWARNING: 'BATCH_EXPANSION_TH' was set, but 'train_rate_non_zero' is None. \",\n",
    "                \"The batch size was not expanded.\",\n",
    "            )\n",
    "        elif train_rate_non_zero < BATCH_EXPANSION_TH:\n",
    "            print(\n",
    "                \"Average non-zero triplet ratio is less than threshold: \",\n",
    "                f\"{train_rate_non_zero} < {BATCH_EXPANSION_TH}\",\n",
    "            )\n",
    "            dataloaders[\"train\"].batch_sampler.expand_batch()\n",
    "\n",
    "    print(\"\\n=> Validating:\\n\")\n",
    "\n",
    "    val_stats, val_rate_non_zero = epoch_loop(\n",
    "        dataloader=dataloaders[\"val\"],\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        phase=\"val\",\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nval_rate_non_zero = {val_rate_non_zero}\")\n",
    "\n",
    "    print(\"\\n=> Testing:\\n\")\n",
    "\n",
    "    recall_at_n, recall_at_one_percent, mean_top1_distance = test(\n",
    "        model=model,\n",
    "        descriptor_key=\"fusion\",\n",
    "        dataloader=dataloaders[\"test\"],\n",
    "        dist_thresh=5.0,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    stats_dict = {}\n",
    "    stats_dict[\"test\"] = {\n",
    "        \"mean_top1_distance\": mean_top1_distance,\n",
    "        \"recall_at_1%\": recall_at_one_percent,\n",
    "        \"recall_at_1\": recall_at_n[0],\n",
    "        \"recall_at_3\": recall_at_n[2],\n",
    "        \"recall_at_5\": recall_at_n[4],\n",
    "        \"recall_at_10\": recall_at_n[9],\n",
    "    }\n",
    "    stats_dict[\"train\"] = train_stats\n",
    "    stats_dict[\"train\"][\"batch_size\"] = train_batch_size\n",
    "\n",
    "    # saving checkpoints\n",
    "    checkpoint_dict = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"stats_dict\": stats_dict,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint_dict, CHECKPOINTS_DIR / f\"epoch_{epoch+1}.pth\")\n",
    "    if recall_at_n[0] > best_recall_at_1:\n",
    "        print(\"Recall@1 improved!\")\n",
    "        torch.save(checkpoint_dict, CHECKPOINTS_DIR / \"best.pth\")\n",
    "        best_recall_at_1 = recall_at_n[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating test set descriptors: 100%|██████████| 116/116 [00:16<00:00,  7.19it/s]\n",
      "Calculating metrics: 100%|██████████| 6/6 [00:00<00:00,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@N:\n",
      "[0.6942526  0.81604247 0.86556882 0.89520911 0.91542813 0.92709596\n",
      " 0.93208602 0.9387087  0.94228657 0.94807692 0.95250074 0.9552532\n",
      " 0.95830611 0.96161686 0.96411157 0.96493305 0.96603619 0.96908523\n",
      " 0.97047456 0.97242462 0.9743485  0.9760437  0.9774291  0.97825379\n",
      " 0.97909571]\n",
      "Mean Recall@1% = 0.9270959598770103\n",
      "Mean top-1 distance = 0.9369876623933786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_at_n, recall_at_one_percent, mean_top1_distance = test(\n",
    "    model=model,\n",
    "    descriptor_key=\"fusion\",\n",
    "    dataloader=dataloaders[\"test\"],\n",
    "    dist_thresh=5.0,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка ответа для загрузки на сервер"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример кода для создания файла сабмита. Вы можете модифицировать пайплайн, например добавить ре-ранжирование кандидатов на основе каких-либо характеристик (например, как в [Path-NetVLAD](https://arxiv.org/abs/2103.01486))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_embeddings(model, descriptor_key, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings_list = []\n",
    "        for data in tqdm(dataloader, desc=\"Calculating test set descriptors\"):\n",
    "            batch, _, _ = data\n",
    "            batch = {e: batch[e].to(device) for e in batch}\n",
    "            batch_embeddings = model(batch)\n",
    "            test_embeddings_list.append(batch_embeddings[descriptor_key].cpu().numpy())\n",
    "        test_embeddings = np.vstack(test_embeddings_list)\n",
    "    return test_embeddings\n",
    "\n",
    "\n",
    "def test_submission(\n",
    "    test_embeddings: np.ndarray, dataset_df: pd.DataFrame, filename: str = \"submission.txt\"\n",
    ") -> None:\n",
    "    \"\"\"Function to create submission txt file.\n",
    "\n",
    "    Args:\n",
    "        test_embeddings (np.ndarray): Array of embeddings.\n",
    "        dataset_df (pd.Dataframe): Test dataset dataframe ('test.csv').\n",
    "        filename (str): Name of the output txt file. Defaults to \"submission.txt\".\n",
    "    \"\"\"\n",
    "    tracks = []\n",
    "\n",
    "    for _, group in dataset_df.groupby(\"track\"):\n",
    "        tracks.append(group.index.to_numpy())\n",
    "    n = 1\n",
    "    ij_permutations = sorted(list(itertools.permutations(range(len(tracks)), 2)))\n",
    "    # ij_permutations = [(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]\n",
    "\n",
    "    submission_lines = []\n",
    "\n",
    "    for i, j in tqdm(ij_permutations, desc=\"Calculating metrics\"):\n",
    "        query_indices = tracks[i]\n",
    "        database_indices = tracks[j]\n",
    "        query_embs = test_embeddings[query_indices]\n",
    "        database_embs = test_embeddings[database_indices]\n",
    "\n",
    "        database_tree = KDTree(database_embs)\n",
    "        _, indices = database_tree.query(query_embs, k=n)\n",
    "\n",
    "        submission_lines.extend(list(database_indices[indices.squeeze()]))\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for l in submission_lines:\n",
    "            f.write(str(l)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating test set descriptors: 100%|██████████| 116/116 [00:16<00:00,  7.11it/s]\n",
      "Calculating metrics: 100%|██████████| 6/6 [00:00<00:00,  7.67it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = extract_embeddings(model, descriptor_key=\"fusion\", dataloader=dataloaders[\"test\"], device=DEVICE)\n",
    "test_submission(embeddings, dataset_df=dataloaders[\"test\"].dataset.dataset_df, filename=\"baseline_submission.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл с сабмитом необходимо загружать на яндекс контест: https://contest.yandex.ru/contest/49118/ \n",
    "\n",
    "Удачи!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
